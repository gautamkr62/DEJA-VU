{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTjaDo2r8niQ",
        "outputId": "b184d9c3-5dec-4078-dc1d-69e222bd8416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/part_two_dataset/\n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "  inflating: dataset/README.md       \n",
            "  inflating: __MACOSX/dataset/._README.md  \n",
            "   creating: dataset/part_one_dataset/\n",
            "  inflating: dataset/part_two_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_two_dataset/._.DS_Store  \n",
            "   creating: dataset/part_two_dataset/train_data/\n",
            "   creating: dataset/part_two_dataset/eval_data/\n",
            "  inflating: dataset/part_one_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_one_dataset/._.DS_Store  \n",
            "   creating: dataset/part_one_dataset/train_data/\n",
            "   creating: dataset/part_one_dataset/eval_data/\n",
            "  inflating: dataset/part_two_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/3_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/3_eval_data.tar.pth  \n"
          ]
        }
      ],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello world\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nl4ASV9_q__",
        "outputId": "03c1c3bd-147a-48e1-cf54-e62bb3cd2a0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_matrix = np.zeros((20, 20))"
      ],
      "metadata": {
        "id": "uyC_08lrQDLg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image  # Import PIL Image\n",
        "\n",
        "# Custom Dataset for datasets with or without labels\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(self, data_array, labels_array=None, transform=None):\n",
        "        self.data = data_array  # NumPy array of images\n",
        "        self.labels = labels_array  # NumPy array of labels or None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Use len instead of size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            # Ensure label is a scalar\n",
        "            if isinstance(label, np.ndarray):\n",
        "                label = int(label)  # Convert NumPy array to scalar\n",
        "            elif torch.is_tensor(label):\n",
        "                label = label.item()\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Function to load dataset from .tar.pth files\n",
        "def load_dataset(file_path):\n",
        "    # Load the .tar.pth file (contains 'data' and 'targets')\n",
        "    data_dict = torch.load(file_path)\n",
        "    data = data_dict['data']  # NumPy array of images\n",
        "    labels = data_dict.get('targets', None)  # Use 'targets' instead of 'labels'\n",
        "    return data, labels\n",
        "\n",
        "# Functions to extract features\n",
        "def extract_features(dataloader, feature_extractor, device):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, label in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = feature_extractor(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs.cpu())\n",
        "            labels.extend(label)  # Use extend instead of append\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.tensor(labels)  # Convert list of labels to tensor\n",
        "    return features, labels\n",
        "\n",
        "def extract_features_unlabeled(dataloader, feature_extractor, device):\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader:\n",
        "            if isinstance(inputs, list) or isinstance(inputs, tuple):\n",
        "                inputs = inputs[0]  # Extract inputs from list/tuple\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = feature_extractor(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs.cpu())\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    return features\n",
        "\n",
        "# Classifier model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=10):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Main training and evaluation function\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Data transformations\n",
        "    data_transforms = transforms.Compose([\n",
        "        # Removed transforms.ToPILImage() since images are already PIL Images\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Feature extractor\n",
        "    feature_extractor = models.resnet152(pretrained=True)\n",
        "    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "\n",
        "    # Paths to datasets\n",
        "    dataset_folder = 'dataset'\n",
        "    part_one_dataset = os.path.join(dataset_folder, 'part_one_dataset')\n",
        "    part_two_dataset = os.path.join(dataset_folder, 'part_two_dataset')\n",
        "\n",
        "    # Load D1 (dataset index 1 in part_one_dataset)\n",
        "    D1_train_data_path = os.path.join(part_one_dataset, 'train_data', '1_train_data.tar.pth')\n",
        "    D1_data, D1_labels = load_dataset(D1_train_data_path)\n",
        "    D1_dataset = CIFARDataset(D1_data, D1_labels, transform=data_transforms)\n",
        "    D1_dataloader = DataLoader(D1_dataset, batch_size=64, shuffle=False)\n",
        "    D1_features, D1_labels = extract_features(D1_dataloader, feature_extractor, device)\n",
        "\n",
        "    # Initialize model f1\n",
        "    input_dim = D1_features.size(1)\n",
        "    f_prev = Classifier(input_dim, num_classes=10).to(device)\n",
        "\n",
        "    # Train f1 on D1\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(f_prev.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    num_epochs = 20\n",
        "    dataset = torch.utils.data.TensorDataset(D1_features, D1_labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    print(\"Training model f1 on D1\")\n",
        "\n",
        "    # Loss components\n",
        "    criterion_ce = nn.CrossEntropyLoss()  # For pseudo-labels\n",
        "    temperature = 1.0  # Distillation temperature\n",
        "    lambda_distill = 0.1  # Weight for distillation loss\n",
        "    lambda_reg = 0.01  # Weight for L2 regularization\n",
        "\n",
        "    theta_prev = {name: param.detach().clone() for name, param in f_prev.named_parameters()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        f_prev.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Current model predictions\n",
        "            outputs_current = f_prev(inputs)\n",
        "            loss_ce = criterion_ce(outputs_current, labels)\n",
        "\n",
        "            # Previous model predictions (soft targets)\n",
        "            with torch.no_grad():\n",
        "                outputs_prev = f_prev(inputs)\n",
        "            soft_targets_prev = torch.softmax(outputs_prev / temperature, dim=1)\n",
        "            soft_targets_current = torch.log_softmax(outputs_current / temperature, dim=1)\n",
        "            loss_distill = -torch.sum(soft_targets_prev * soft_targets_current) / inputs.size(0)\n",
        "\n",
        "            # L2 regularization\n",
        "            l2_reg = 0.0\n",
        "            for name, param in f_prev.named_parameters():\n",
        "                l2_reg += torch.norm(param - theta_prev[name].to(device)) ** 2\n",
        "\n",
        "            # Combined loss\n",
        "            loss = loss_ce + lambda_distill * loss_distill + lambda_reg * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}')\n",
        "\n",
        "    # Initialize accuracy matrix\n",
        "    # accuracy_matrix = np.zeros((20, 20))\n",
        "\n",
        "    # Loop over datasets D2 to D20\n",
        "    for i in range(1, 21):\n",
        "        print(f'\\nProcessing Dataset D{i}')\n",
        "\n",
        "        if i <= 10:\n",
        "            # Datasets D2 to D10 in part_one_dataset\n",
        "            train_data_path = os.path.join(part_one_dataset, 'train_data', f'{i}_train_data.tar.pth')\n",
        "        else:\n",
        "            # Datasets D11 to D20 in part_two_dataset\n",
        "            idx = i - 10  # Since files are numbered from 1 to 10\n",
        "            train_data_path = os.path.join(part_two_dataset, 'train_data', f'{idx}_train_data.tar.pth')\n",
        "\n",
        "        # Load Di\n",
        "        Di_data, _ = load_dataset(train_data_path)\n",
        "        Di_dataset = CIFARDataset(Di_data, transform=data_transforms)\n",
        "        Di_dataloader = DataLoader(Di_dataset, batch_size=32, shuffle=False)\n",
        "        Di_features = extract_features_unlabeled(Di_dataloader, feature_extractor, device)\n",
        "\n",
        "        # Predict labels using f_prev\n",
        "        f_prev.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = f_prev(Di_features.to(device))\n",
        "            _, predicted_labels = torch.max(outputs, 1)\n",
        "            predicted_labels = predicted_labels.cpu()\n",
        "\n",
        "        # Prepare dataset with pseudo-labels\n",
        "        dataset = torch.utils.data.TensorDataset(Di_features, predicted_labels)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Initialize current model fi\n",
        "        fi = Classifier(input_dim, num_classes=10).to(device)\n",
        "        fi.load_state_dict(f_prev.state_dict())\n",
        "\n",
        "# Store the parameters of the previous model (f_prev)\n",
        "        theta_prev = {name: param.detach().clone() for name, param in f_prev.named_parameters()}\n",
        "\n",
        "\n",
        "        # Regularization parameter\n",
        "        lambda_reg = 1.0 if i <= 10 else 0.1  # Adjusted for potential distribution shift\n",
        "\n",
        "        # Store previous parameters\n",
        "        for name, param in f_prev.named_parameters():\n",
        "            theta_prev[name] = param.detach().clone()\n",
        "\n",
        "\n",
        "                # Initialize optimizer and scheduler\n",
        "        optimizer = optim.Adam(fi.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "        # Training fi on Di with pseudo-labels\n",
        "        print(f\"Training model f{i} on D{i}\")\n",
        "        for epoch in range(num_epochs):\n",
        "            fi.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Predictions from f_prev (teacher) for distillation\n",
        "                with torch.no_grad():\n",
        "                    outputs_prev = f_prev(inputs)\n",
        "\n",
        "                # Predictions from fi (student)\n",
        "                outputs_current = fi(inputs)\n",
        "\n",
        "                # Cross-entropy loss\n",
        "                loss_ce = criterion(outputs_current, labels)\n",
        "\n",
        "                # Distillation loss\n",
        "                soft_targets_prev = torch.softmax(outputs_prev / temperature, dim=1)\n",
        "                soft_targets_current = torch.log_softmax(outputs_current / temperature, dim=1)\n",
        "                loss_distill = -torch.sum(soft_targets_prev * soft_targets_current) / inputs.size(0)\n",
        "\n",
        "                # Regularization loss\n",
        "                l2_reg = 0.0\n",
        "                for name, param in fi.named_parameters():\n",
        "                    l2_reg += torch.norm(param - theta_prev[name].to(device)) ** 2\n",
        "\n",
        "                # Combined loss\n",
        "                loss = loss_ce + lambda_distill * loss_distill + lambda_reg * l2_reg\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Evaluation on D_hat datasets\n",
        "        for j in range(1, i+1):\n",
        "            if j <= 10:\n",
        "                # D_hat datasets in part_one_dataset\n",
        "                eval_data_path = os.path.join(part_one_dataset, 'eval_data', f'{j}_eval_data.tar.pth')\n",
        "                D_hat_data, D_hat_labels = load_dataset(eval_data_path)\n",
        "            else:\n",
        "                # D_hat datasets in part_two_dataset\n",
        "                idx = j - 10\n",
        "                eval_data_path = os.path.join(part_two_dataset, 'eval_data', f'{idx}_eval_data.tar.pth')\n",
        "                D_hat_data, D_hat_labels = load_dataset(eval_data_path)\n",
        "\n",
        "            D_hat_dataset = CIFARDataset(D_hat_data, D_hat_labels, transform=data_transforms)\n",
        "            D_hat_dataloader = DataLoader(D_hat_dataset, batch_size=64, shuffle=False)\n",
        "            D_hat_features, D_hat_labels = extract_features(D_hat_dataloader, feature_extractor, device)\n",
        "\n",
        "            fi.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = fi(D_hat_features.to(device))\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct = (predicted.cpu() == D_hat_labels).sum().item()\n",
        "                total = D_hat_labels.size(0)\n",
        "                accuracy = correct / total\n",
        "                print(f'Model f{i} Accuracy on D_hat{j}: {accuracy*100:.2f}%')\n",
        "\n",
        "                # Store accuracy in matrix\n",
        "                accuracy_matrix[i-1][j-1] = accuracy * 100\n",
        "\n",
        "        # Update f_prev to fi\n",
        "        f_prev = fi\n",
        "\n",
        "    # Display accuracy matrix\n",
        "    print('\\nAccuracy Matrix:')\n",
        "    for i in range(20):\n",
        "        accuracies = ['{:.2f}'.format(acc) for acc in accuracy_matrix[i][:i+1]]\n",
        "        print(f'Model f{i+1}: ' + ', '.join(accuracies))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItVRlmswAX4i",
        "outputId": "adb64df5-8a61-4189-83d4-ed613d6e252d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-4-afa5910612c0>:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_dict = torch.load(file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model f1 on D1\n",
            "Epoch 1/20, Loss: 0.9476\n",
            "Epoch 2/20, Loss: 0.4819\n",
            "Epoch 3/20, Loss: 0.4343\n",
            "Epoch 4/20, Loss: 0.4048\n",
            "Epoch 5/20, Loss: 0.3727\n",
            "Epoch 6/20, Loss: 0.4595\n",
            "Epoch 7/20, Loss: 0.4250\n",
            "Epoch 8/20, Loss: 0.4247\n",
            "Epoch 9/20, Loss: 0.3576\n",
            "Epoch 10/20, Loss: 0.3373\n",
            "Epoch 11/20, Loss: 0.3771\n",
            "Epoch 12/20, Loss: 0.3610\n",
            "Epoch 13/20, Loss: 0.4049\n",
            "Epoch 14/20, Loss: 0.3993\n",
            "Epoch 15/20, Loss: 0.3586\n",
            "Epoch 16/20, Loss: 0.3411\n",
            "Epoch 17/20, Loss: 0.3301\n",
            "Epoch 18/20, Loss: 0.3388\n",
            "Epoch 19/20, Loss: 0.3862\n",
            "Epoch 20/20, Loss: 0.3366\n",
            "\n",
            "Processing Dataset D1\n",
            "Training model f1 on D1\n",
            "Epoch 1/20, Loss: 0.1873, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1799, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1828, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1897, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1944, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1828, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1834, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1771, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1759, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1724, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1785, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1811, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1716, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1642, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1618, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1610, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1599, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1682, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1616, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1656, LR: 0.000000\n",
            "Model f1 Accuracy on D_hat1: 89.48%\n",
            "\n",
            "Processing Dataset D2\n",
            "Training model f2 on D2\n",
            "Epoch 1/20, Loss: 0.2016, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2035, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2090, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2130, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2051, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.2076, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1999, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2011, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1982, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1948, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1914, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1897, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1893, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1852, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1892, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1830, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1831, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1791, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1807, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1875, LR: 0.000000\n",
            "Model f2 Accuracy on D_hat1: 89.60%\n",
            "Model f2 Accuracy on D_hat2: 89.56%\n",
            "\n",
            "Processing Dataset D3\n",
            "Training model f3 on D3\n",
            "Epoch 1/20, Loss: 0.2305, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2244, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2088, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2178, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2104, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1976, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.2037, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2020, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.2027, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1970, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1951, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1958, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1891, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1911, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1913, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1939, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1846, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1903, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1815, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1828, LR: 0.000000\n",
            "Model f3 Accuracy on D_hat1: 89.68%\n",
            "Model f3 Accuracy on D_hat2: 89.68%\n",
            "Model f3 Accuracy on D_hat3: 89.80%\n",
            "\n",
            "Processing Dataset D4\n",
            "Training model f4 on D4\n",
            "Epoch 1/20, Loss: 0.2165, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2025, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2170, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2089, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2127, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.2169, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.2112, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2073, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.2032, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.2029, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.2030, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.2072, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.2063, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1928, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1909, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1878, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1879, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1899, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1819, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1811, LR: 0.000000\n",
            "Model f4 Accuracy on D_hat1: 89.56%\n",
            "Model f4 Accuracy on D_hat2: 89.48%\n",
            "Model f4 Accuracy on D_hat3: 89.72%\n",
            "Model f4 Accuracy on D_hat4: 89.12%\n",
            "\n",
            "Processing Dataset D5\n",
            "Training model f5 on D5\n",
            "Epoch 1/20, Loss: 0.2086, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1999, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2015, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1951, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1897, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1932, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1988, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2001, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1888, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1883, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1919, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1815, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1808, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1874, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1801, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1750, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1805, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1784, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1743, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1719, LR: 0.000000\n",
            "Model f5 Accuracy on D_hat1: 89.60%\n",
            "Model f5 Accuracy on D_hat2: 89.40%\n",
            "Model f5 Accuracy on D_hat3: 89.84%\n",
            "Model f5 Accuracy on D_hat4: 89.24%\n",
            "Model f5 Accuracy on D_hat5: 88.52%\n",
            "\n",
            "Processing Dataset D6\n",
            "Training model f6 on D6\n",
            "Epoch 1/20, Loss: 0.1989, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1927, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1960, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2029, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1988, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1908, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1910, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1905, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1791, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1779, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1762, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1747, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1697, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1756, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1771, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1726, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1672, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1683, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1661, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1630, LR: 0.000000\n",
            "Model f6 Accuracy on D_hat1: 89.44%\n",
            "Model f6 Accuracy on D_hat2: 89.56%\n",
            "Model f6 Accuracy on D_hat3: 89.80%\n",
            "Model f6 Accuracy on D_hat4: 89.20%\n",
            "Model f6 Accuracy on D_hat5: 88.40%\n",
            "Model f6 Accuracy on D_hat6: 89.16%\n",
            "\n",
            "Processing Dataset D7\n",
            "Training model f7 on D7\n",
            "Epoch 1/20, Loss: 0.2010, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1883, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1930, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1913, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1849, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1795, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1951, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2156, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1882, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1779, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1810, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1736, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1737, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1666, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1758, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1687, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1673, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1783, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1700, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1734, LR: 0.000000\n",
            "Model f7 Accuracy on D_hat1: 89.44%\n",
            "Model f7 Accuracy on D_hat2: 89.68%\n",
            "Model f7 Accuracy on D_hat3: 89.76%\n",
            "Model f7 Accuracy on D_hat4: 89.32%\n",
            "Model f7 Accuracy on D_hat5: 88.52%\n",
            "Model f7 Accuracy on D_hat6: 89.20%\n",
            "Model f7 Accuracy on D_hat7: 88.96%\n",
            "\n",
            "Processing Dataset D8\n",
            "Training model f8 on D8\n",
            "Epoch 1/20, Loss: 0.1936, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1969, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1835, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1849, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1812, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1792, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1821, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1742, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1806, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1768, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1917, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1754, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1748, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1768, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1691, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1626, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1656, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1634, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1641, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1650, LR: 0.000000\n",
            "Model f8 Accuracy on D_hat1: 89.28%\n",
            "Model f8 Accuracy on D_hat2: 90.00%\n",
            "Model f8 Accuracy on D_hat3: 89.96%\n",
            "Model f8 Accuracy on D_hat4: 89.56%\n",
            "Model f8 Accuracy on D_hat5: 88.72%\n",
            "Model f8 Accuracy on D_hat6: 89.04%\n",
            "Model f8 Accuracy on D_hat7: 89.04%\n",
            "Model f8 Accuracy on D_hat8: 88.76%\n",
            "\n",
            "Processing Dataset D9\n",
            "Training model f9 on D9\n",
            "Epoch 1/20, Loss: 0.1862, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1759, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1691, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1807, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1866, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1711, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1716, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1741, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1690, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1649, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1724, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1739, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1621, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1603, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1574, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1549, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1563, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1533, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1518, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1562, LR: 0.000000\n",
            "Model f9 Accuracy on D_hat1: 89.20%\n",
            "Model f9 Accuracy on D_hat2: 90.00%\n",
            "Model f9 Accuracy on D_hat3: 89.76%\n",
            "Model f9 Accuracy on D_hat4: 89.44%\n",
            "Model f9 Accuracy on D_hat5: 88.36%\n",
            "Model f9 Accuracy on D_hat6: 88.64%\n",
            "Model f9 Accuracy on D_hat7: 89.20%\n",
            "Model f9 Accuracy on D_hat8: 88.52%\n",
            "Model f9 Accuracy on D_hat9: 88.76%\n",
            "\n",
            "Processing Dataset D10\n",
            "Training model f10 on D10\n",
            "Epoch 1/20, Loss: 0.1830, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1741, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1887, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1887, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1817, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1772, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1723, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1647, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1678, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1689, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1650, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1640, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1601, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1589, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1574, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1597, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1589, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1566, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1552, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1579, LR: 0.000000\n",
            "Model f10 Accuracy on D_hat1: 89.28%\n",
            "Model f10 Accuracy on D_hat2: 89.96%\n",
            "Model f10 Accuracy on D_hat3: 89.68%\n",
            "Model f10 Accuracy on D_hat4: 89.32%\n",
            "Model f10 Accuracy on D_hat5: 88.36%\n",
            "Model f10 Accuracy on D_hat6: 88.72%\n",
            "Model f10 Accuracy on D_hat7: 89.08%\n",
            "Model f10 Accuracy on D_hat8: 88.76%\n",
            "Model f10 Accuracy on D_hat9: 89.00%\n",
            "Model f10 Accuracy on D_hat10: 89.52%\n",
            "\n",
            "Processing Dataset D11\n",
            "Training model f11 on D11\n",
            "Epoch 1/20, Loss: 0.3687, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.3468, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.3563, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.3564, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.3410, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.3521, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.3460, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.3607, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.3349, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.3305, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.3342, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.3412, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.3360, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.3269, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.3258, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.3179, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.3231, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.3163, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.3142, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.3190, LR: 0.000000\n",
            "Model f11 Accuracy on D_hat1: 88.92%\n",
            "Model f11 Accuracy on D_hat2: 89.48%\n",
            "Model f11 Accuracy on D_hat3: 89.60%\n",
            "Model f11 Accuracy on D_hat4: 89.24%\n",
            "Model f11 Accuracy on D_hat5: 88.28%\n",
            "Model f11 Accuracy on D_hat6: 88.56%\n",
            "Model f11 Accuracy on D_hat7: 88.76%\n",
            "Model f11 Accuracy on D_hat8: 88.64%\n",
            "Model f11 Accuracy on D_hat9: 88.80%\n",
            "Model f11 Accuracy on D_hat10: 89.08%\n",
            "Model f11 Accuracy on D_hat11: 71.24%\n",
            "\n",
            "Processing Dataset D12\n",
            "Training model f12 on D12\n",
            "Epoch 1/20, Loss: 0.3820, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.3473, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.3434, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.3623, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.3444, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.3442, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.3523, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.3371, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.3336, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.3368, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.3381, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.3299, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.3289, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.3348, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.3270, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.3257, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.3279, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.3198, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.3243, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.3144, LR: 0.000000\n",
            "Model f12 Accuracy on D_hat1: 88.52%\n",
            "Model f12 Accuracy on D_hat2: 89.16%\n",
            "Model f12 Accuracy on D_hat3: 88.64%\n",
            "Model f12 Accuracy on D_hat4: 88.76%\n",
            "Model f12 Accuracy on D_hat5: 88.20%\n",
            "Model f12 Accuracy on D_hat6: 88.32%\n",
            "Model f12 Accuracy on D_hat7: 87.92%\n",
            "Model f12 Accuracy on D_hat8: 88.44%\n",
            "Model f12 Accuracy on D_hat9: 88.20%\n",
            "Model f12 Accuracy on D_hat10: 88.56%\n",
            "Model f12 Accuracy on D_hat11: 70.92%\n",
            "Model f12 Accuracy on D_hat12: 57.80%\n",
            "\n",
            "Processing Dataset D13\n",
            "Training model f13 on D13\n",
            "Epoch 1/20, Loss: 0.2554, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2420, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2324, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2208, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2139, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.2140, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.2155, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2168, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.2087, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.2162, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.2081, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.2058, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.2062, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.2049, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.2025, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1989, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1970, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.2012, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1993, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.2042, LR: 0.000000\n",
            "Model f13 Accuracy on D_hat1: 88.24%\n",
            "Model f13 Accuracy on D_hat2: 89.44%\n",
            "Model f13 Accuracy on D_hat3: 88.36%\n",
            "Model f13 Accuracy on D_hat4: 88.44%\n",
            "Model f13 Accuracy on D_hat5: 87.96%\n",
            "Model f13 Accuracy on D_hat6: 88.00%\n",
            "Model f13 Accuracy on D_hat7: 87.60%\n",
            "Model f13 Accuracy on D_hat8: 88.20%\n",
            "Model f13 Accuracy on D_hat9: 88.00%\n",
            "Model f13 Accuracy on D_hat10: 88.64%\n",
            "Model f13 Accuracy on D_hat11: 71.00%\n",
            "Model f13 Accuracy on D_hat12: 58.00%\n",
            "Model f13 Accuracy on D_hat13: 77.00%\n",
            "\n",
            "Processing Dataset D14\n",
            "Training model f14 on D14\n",
            "Epoch 1/20, Loss: 0.2297, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2093, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2002, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2070, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2045, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.2088, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.2097, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1964, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.2017, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1910, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1885, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1909, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1969, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1902, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1932, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1865, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1848, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1810, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1790, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1793, LR: 0.000000\n",
            "Model f14 Accuracy on D_hat1: 88.20%\n",
            "Model f14 Accuracy on D_hat2: 89.12%\n",
            "Model f14 Accuracy on D_hat3: 88.24%\n",
            "Model f14 Accuracy on D_hat4: 88.52%\n",
            "Model f14 Accuracy on D_hat5: 87.80%\n",
            "Model f14 Accuracy on D_hat6: 87.96%\n",
            "Model f14 Accuracy on D_hat7: 87.60%\n",
            "Model f14 Accuracy on D_hat8: 88.16%\n",
            "Model f14 Accuracy on D_hat9: 87.92%\n",
            "Model f14 Accuracy on D_hat10: 88.16%\n",
            "Model f14 Accuracy on D_hat11: 71.48%\n",
            "Model f14 Accuracy on D_hat12: 57.92%\n",
            "Model f14 Accuracy on D_hat13: 76.72%\n",
            "Model f14 Accuracy on D_hat14: 75.52%\n",
            "\n",
            "Processing Dataset D15\n",
            "Training model f15 on D15\n",
            "Epoch 1/20, Loss: 0.1256, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1148, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1161, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1244, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1334, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1139, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1107, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1116, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1176, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1065, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1080, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1082, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1163, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1075, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1055, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.0979, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1013, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.0991, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.0958, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.0997, LR: 0.000000\n",
            "Model f15 Accuracy on D_hat1: 88.32%\n",
            "Model f15 Accuracy on D_hat2: 89.00%\n",
            "Model f15 Accuracy on D_hat3: 88.28%\n",
            "Model f15 Accuracy on D_hat4: 88.24%\n",
            "Model f15 Accuracy on D_hat5: 87.64%\n",
            "Model f15 Accuracy on D_hat6: 88.04%\n",
            "Model f15 Accuracy on D_hat7: 87.60%\n",
            "Model f15 Accuracy on D_hat8: 87.72%\n",
            "Model f15 Accuracy on D_hat9: 87.44%\n",
            "Model f15 Accuracy on D_hat10: 88.40%\n",
            "Model f15 Accuracy on D_hat11: 71.44%\n",
            "Model f15 Accuracy on D_hat12: 58.36%\n",
            "Model f15 Accuracy on D_hat13: 76.80%\n",
            "Model f15 Accuracy on D_hat14: 75.52%\n",
            "Model f15 Accuracy on D_hat15: 87.24%\n",
            "\n",
            "Processing Dataset D16\n",
            "Training model f16 on D16\n",
            "Epoch 1/20, Loss: 0.2295, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.2233, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2117, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.2130, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.2066, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.2033, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.2075, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.2010, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1997, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1981, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1987, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1946, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1897, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1880, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1881, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1850, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1810, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1811, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1831, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1774, LR: 0.000000\n",
            "Model f16 Accuracy on D_hat1: 87.88%\n",
            "Model f16 Accuracy on D_hat2: 88.60%\n",
            "Model f16 Accuracy on D_hat3: 88.12%\n",
            "Model f16 Accuracy on D_hat4: 88.16%\n",
            "Model f16 Accuracy on D_hat5: 87.32%\n",
            "Model f16 Accuracy on D_hat6: 87.96%\n",
            "Model f16 Accuracy on D_hat7: 87.44%\n",
            "Model f16 Accuracy on D_hat8: 87.84%\n",
            "Model f16 Accuracy on D_hat9: 87.56%\n",
            "Model f16 Accuracy on D_hat10: 88.04%\n",
            "Model f16 Accuracy on D_hat11: 70.64%\n",
            "Model f16 Accuracy on D_hat12: 58.28%\n",
            "Model f16 Accuracy on D_hat13: 76.28%\n",
            "Model f16 Accuracy on D_hat14: 75.28%\n",
            "Model f16 Accuracy on D_hat15: 87.04%\n",
            "Model f16 Accuracy on D_hat16: 72.84%\n",
            "\n",
            "Processing Dataset D17\n",
            "Training model f17 on D17\n",
            "Epoch 1/20, Loss: 0.2147, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1976, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.2004, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1922, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1949, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1948, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1911, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1863, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1960, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1809, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1816, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1839, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1790, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1810, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1758, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1713, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1706, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1688, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1697, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1725, LR: 0.000000\n",
            "Model f17 Accuracy on D_hat1: 87.84%\n",
            "Model f17 Accuracy on D_hat2: 88.60%\n",
            "Model f17 Accuracy on D_hat3: 87.88%\n",
            "Model f17 Accuracy on D_hat4: 88.20%\n",
            "Model f17 Accuracy on D_hat5: 87.28%\n",
            "Model f17 Accuracy on D_hat6: 87.76%\n",
            "Model f17 Accuracy on D_hat7: 87.36%\n",
            "Model f17 Accuracy on D_hat8: 87.64%\n",
            "Model f17 Accuracy on D_hat9: 87.48%\n",
            "Model f17 Accuracy on D_hat10: 87.76%\n",
            "Model f17 Accuracy on D_hat11: 70.76%\n",
            "Model f17 Accuracy on D_hat12: 58.68%\n",
            "Model f17 Accuracy on D_hat13: 76.80%\n",
            "Model f17 Accuracy on D_hat14: 75.20%\n",
            "Model f17 Accuracy on D_hat15: 87.24%\n",
            "Model f17 Accuracy on D_hat16: 72.72%\n",
            "Model f17 Accuracy on D_hat17: 71.08%\n",
            "\n",
            "Processing Dataset D18\n",
            "Training model f18 on D18\n",
            "Epoch 1/20, Loss: 0.2176, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1929, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1903, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1958, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1990, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1891, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1909, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1846, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1897, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1868, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1857, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1816, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1806, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1834, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1776, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1714, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1711, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1690, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1754, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1743, LR: 0.000000\n",
            "Model f18 Accuracy on D_hat1: 87.88%\n",
            "Model f18 Accuracy on D_hat2: 89.00%\n",
            "Model f18 Accuracy on D_hat3: 87.76%\n",
            "Model f18 Accuracy on D_hat4: 88.16%\n",
            "Model f18 Accuracy on D_hat5: 87.20%\n",
            "Model f18 Accuracy on D_hat6: 87.84%\n",
            "Model f18 Accuracy on D_hat7: 87.28%\n",
            "Model f18 Accuracy on D_hat8: 87.64%\n",
            "Model f18 Accuracy on D_hat9: 87.24%\n",
            "Model f18 Accuracy on D_hat10: 87.60%\n",
            "Model f18 Accuracy on D_hat11: 70.84%\n",
            "Model f18 Accuracy on D_hat12: 59.00%\n",
            "Model f18 Accuracy on D_hat13: 76.68%\n",
            "Model f18 Accuracy on D_hat14: 75.52%\n",
            "Model f18 Accuracy on D_hat15: 87.12%\n",
            "Model f18 Accuracy on D_hat16: 72.84%\n",
            "Model f18 Accuracy on D_hat17: 71.36%\n",
            "Model f18 Accuracy on D_hat18: 73.64%\n",
            "\n",
            "Processing Dataset D19\n",
            "Training model f19 on D19\n",
            "Epoch 1/20, Loss: 0.2077, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1920, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1931, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1912, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1853, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1829, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1880, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1853, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1786, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1838, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1863, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1820, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1789, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1742, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1743, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1711, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1804, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1725, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1729, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1699, LR: 0.000000\n",
            "Model f19 Accuracy on D_hat1: 87.68%\n",
            "Model f19 Accuracy on D_hat2: 88.88%\n",
            "Model f19 Accuracy on D_hat3: 87.52%\n",
            "Model f19 Accuracy on D_hat4: 87.92%\n",
            "Model f19 Accuracy on D_hat5: 87.04%\n",
            "Model f19 Accuracy on D_hat6: 87.76%\n",
            "Model f19 Accuracy on D_hat7: 87.20%\n",
            "Model f19 Accuracy on D_hat8: 87.48%\n",
            "Model f19 Accuracy on D_hat9: 87.08%\n",
            "Model f19 Accuracy on D_hat10: 87.44%\n",
            "Model f19 Accuracy on D_hat11: 70.64%\n",
            "Model f19 Accuracy on D_hat12: 58.68%\n",
            "Model f19 Accuracy on D_hat13: 76.16%\n",
            "Model f19 Accuracy on D_hat14: 75.48%\n",
            "Model f19 Accuracy on D_hat15: 87.20%\n",
            "Model f19 Accuracy on D_hat16: 72.36%\n",
            "Model f19 Accuracy on D_hat17: 70.80%\n",
            "Model f19 Accuracy on D_hat18: 73.12%\n",
            "Model f19 Accuracy on D_hat19: 67.24%\n",
            "\n",
            "Processing Dataset D20\n",
            "Training model f20 on D20\n",
            "Epoch 1/20, Loss: 0.1443, LR: 0.000994\n",
            "Epoch 2/20, Loss: 0.1218, LR: 0.000976\n",
            "Epoch 3/20, Loss: 0.1157, LR: 0.000946\n",
            "Epoch 4/20, Loss: 0.1258, LR: 0.000905\n",
            "Epoch 5/20, Loss: 0.1305, LR: 0.000854\n",
            "Epoch 6/20, Loss: 0.1271, LR: 0.000794\n",
            "Epoch 7/20, Loss: 0.1238, LR: 0.000727\n",
            "Epoch 8/20, Loss: 0.1234, LR: 0.000655\n",
            "Epoch 9/20, Loss: 0.1222, LR: 0.000578\n",
            "Epoch 10/20, Loss: 0.1104, LR: 0.000500\n",
            "Epoch 11/20, Loss: 0.1096, LR: 0.000422\n",
            "Epoch 12/20, Loss: 0.1072, LR: 0.000345\n",
            "Epoch 13/20, Loss: 0.1070, LR: 0.000273\n",
            "Epoch 14/20, Loss: 0.1040, LR: 0.000206\n",
            "Epoch 15/20, Loss: 0.1050, LR: 0.000146\n",
            "Epoch 16/20, Loss: 0.1078, LR: 0.000095\n",
            "Epoch 17/20, Loss: 0.1063, LR: 0.000054\n",
            "Epoch 18/20, Loss: 0.1040, LR: 0.000024\n",
            "Epoch 19/20, Loss: 0.1019, LR: 0.000006\n",
            "Epoch 20/20, Loss: 0.1015, LR: 0.000000\n",
            "Model f20 Accuracy on D_hat1: 87.40%\n",
            "Model f20 Accuracy on D_hat2: 88.60%\n",
            "Model f20 Accuracy on D_hat3: 86.92%\n",
            "Model f20 Accuracy on D_hat4: 87.64%\n",
            "Model f20 Accuracy on D_hat5: 86.92%\n",
            "Model f20 Accuracy on D_hat6: 87.64%\n",
            "Model f20 Accuracy on D_hat7: 86.72%\n",
            "Model f20 Accuracy on D_hat8: 87.40%\n",
            "Model f20 Accuracy on D_hat9: 87.00%\n",
            "Model f20 Accuracy on D_hat10: 87.44%\n",
            "Model f20 Accuracy on D_hat11: 70.24%\n",
            "Model f20 Accuracy on D_hat12: 58.76%\n",
            "Model f20 Accuracy on D_hat13: 76.44%\n",
            "Model f20 Accuracy on D_hat14: 74.92%\n",
            "Model f20 Accuracy on D_hat15: 86.88%\n",
            "Model f20 Accuracy on D_hat16: 72.76%\n",
            "Model f20 Accuracy on D_hat17: 70.56%\n",
            "Model f20 Accuracy on D_hat18: 72.96%\n",
            "Model f20 Accuracy on D_hat19: 66.96%\n",
            "Model f20 Accuracy on D_hat20: 83.80%\n",
            "\n",
            "Accuracy Matrix:\n",
            "Model f1: 89.48\n",
            "Model f2: 89.60, 89.56\n",
            "Model f3: 89.68, 89.68, 89.80\n",
            "Model f4: 89.56, 89.48, 89.72, 89.12\n",
            "Model f5: 89.60, 89.40, 89.84, 89.24, 88.52\n",
            "Model f6: 89.44, 89.56, 89.80, 89.20, 88.40, 89.16\n",
            "Model f7: 89.44, 89.68, 89.76, 89.32, 88.52, 89.20, 88.96\n",
            "Model f8: 89.28, 90.00, 89.96, 89.56, 88.72, 89.04, 89.04, 88.76\n",
            "Model f9: 89.20, 90.00, 89.76, 89.44, 88.36, 88.64, 89.20, 88.52, 88.76\n",
            "Model f10: 89.28, 89.96, 89.68, 89.32, 88.36, 88.72, 89.08, 88.76, 89.00, 89.52\n",
            "Model f11: 88.92, 89.48, 89.60, 89.24, 88.28, 88.56, 88.76, 88.64, 88.80, 89.08, 71.24\n",
            "Model f12: 88.52, 89.16, 88.64, 88.76, 88.20, 88.32, 87.92, 88.44, 88.20, 88.56, 70.92, 57.80\n",
            "Model f13: 88.24, 89.44, 88.36, 88.44, 87.96, 88.00, 87.60, 88.20, 88.00, 88.64, 71.00, 58.00, 77.00\n",
            "Model f14: 88.20, 89.12, 88.24, 88.52, 87.80, 87.96, 87.60, 88.16, 87.92, 88.16, 71.48, 57.92, 76.72, 75.52\n",
            "Model f15: 88.32, 89.00, 88.28, 88.24, 87.64, 88.04, 87.60, 87.72, 87.44, 88.40, 71.44, 58.36, 76.80, 75.52, 87.24\n",
            "Model f16: 87.88, 88.60, 88.12, 88.16, 87.32, 87.96, 87.44, 87.84, 87.56, 88.04, 70.64, 58.28, 76.28, 75.28, 87.04, 72.84\n",
            "Model f17: 87.84, 88.60, 87.88, 88.20, 87.28, 87.76, 87.36, 87.64, 87.48, 87.76, 70.76, 58.68, 76.80, 75.20, 87.24, 72.72, 71.08\n",
            "Model f18: 87.88, 89.00, 87.76, 88.16, 87.20, 87.84, 87.28, 87.64, 87.24, 87.60, 70.84, 59.00, 76.68, 75.52, 87.12, 72.84, 71.36, 73.64\n",
            "Model f19: 87.68, 88.88, 87.52, 87.92, 87.04, 87.76, 87.20, 87.48, 87.08, 87.44, 70.64, 58.68, 76.16, 75.48, 87.20, 72.36, 70.80, 73.12, 67.24\n",
            "Model f20: 87.40, 88.60, 86.92, 87.64, 86.92, 87.64, 86.72, 87.40, 87.00, 87.44, 70.24, 58.76, 76.44, 74.92, 86.88, 72.76, 70.56, 72.96, 66.96, 83.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5pLGX4EiaVJ",
        "outputId": "7740a784-ee8c-483f-ec4f-f415d938376f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[89.48  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.6  89.56  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.68 89.68 89.8   0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.56 89.48 89.72 89.12  0.    0.    0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.6  89.4  89.84 89.24 88.52  0.    0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.44 89.56 89.8  89.2  88.4  89.16  0.    0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.44 89.68 89.76 89.32 88.52 89.2  88.96  0.    0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.28 90.   89.96 89.56 88.72 89.04 89.04 88.76  0.    0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.2  90.   89.76 89.44 88.36 88.64 89.2  88.52 88.76  0.    0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [89.28 89.96 89.68 89.32 88.36 88.72 89.08 88.76 89.   89.52  0.    0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [88.92 89.48 89.6  89.24 88.28 88.56 88.76 88.64 88.8  89.08 71.24  0.\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [88.52 89.16 88.64 88.76 88.2  88.32 87.92 88.44 88.2  88.56 70.92 57.8\n",
            "   0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [88.24 89.44 88.36 88.44 87.96 88.   87.6  88.2  88.   88.64 71.   58.\n",
            "  77.    0.    0.    0.    0.    0.    0.    0.  ]\n",
            " [88.2  89.12 88.24 88.52 87.8  87.96 87.6  88.16 87.92 88.16 71.48 57.92\n",
            "  76.72 75.52  0.    0.    0.    0.    0.    0.  ]\n",
            " [88.32 89.   88.28 88.24 87.64 88.04 87.6  87.72 87.44 88.4  71.44 58.36\n",
            "  76.8  75.52 87.24  0.    0.    0.    0.    0.  ]\n",
            " [87.88 88.6  88.12 88.16 87.32 87.96 87.44 87.84 87.56 88.04 70.64 58.28\n",
            "  76.28 75.28 87.04 72.84  0.    0.    0.    0.  ]\n",
            " [87.84 88.6  87.88 88.2  87.28 87.76 87.36 87.64 87.48 87.76 70.76 58.68\n",
            "  76.8  75.2  87.24 72.72 71.08  0.    0.    0.  ]\n",
            " [87.88 89.   87.76 88.16 87.2  87.84 87.28 87.64 87.24 87.6  70.84 59.\n",
            "  76.68 75.52 87.12 72.84 71.36 73.64  0.    0.  ]\n",
            " [87.68 88.88 87.52 87.92 87.04 87.76 87.2  87.48 87.08 87.44 70.64 58.68\n",
            "  76.16 75.48 87.2  72.36 70.8  73.12 67.24  0.  ]\n",
            " [87.4  88.6  86.92 87.64 86.92 87.64 86.72 87.4  87.   87.44 70.24 58.76\n",
            "  76.44 74.92 86.88 72.76 70.56 72.96 66.96 83.8 ]]\n"
          ]
        }
      ]
    }
  ]
}