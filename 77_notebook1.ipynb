{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "accuracy_matrix = np.zeros((20, 20))"
      ],
      "metadata": {
        "id": "cpX9etP5q0MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGSF0Fn4qnRO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image  # Import PIL Image\n",
        "\n",
        "# Custom Dataset for datasets with or without labels\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(self, data_array, labels_array=None, transform=None):\n",
        "        self.data = data_array  # NumPy array of images\n",
        "        self.labels = labels_array  # NumPy array of labels or None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Use len instead of size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[idx]\n",
        "            # Ensure label is a scalar\n",
        "            if isinstance(label, np.ndarray):\n",
        "                label = int(label)  # Convert NumPy array to scalar\n",
        "            elif torch.is_tensor(label):\n",
        "                label = label.item()\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Function to load dataset from .tar.pth files\n",
        "def load_dataset(file_path):\n",
        "    # Load the .tar.pth file (contains 'data' and 'targets')\n",
        "    data_dict = torch.load(file_path)\n",
        "    data = data_dict['data']  # NumPy array of images\n",
        "    labels = data_dict.get('targets', None)  # Use 'targets' instead of 'labels'\n",
        "    return data, labels\n",
        "\n",
        "# Functions to extract features\n",
        "def extract_features(dataloader, feature_extractor, device):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, label in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = feature_extractor(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs.cpu())\n",
        "            labels.extend(label)  # Use extend instead of append\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.tensor(labels)  # Convert list of labels to tensor\n",
        "    return features, labels\n",
        "\n",
        "def extract_features_unlabeled(dataloader, feature_extractor, device):\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs in dataloader:\n",
        "            if isinstance(inputs, list) or isinstance(inputs, tuple):\n",
        "                inputs = inputs[0]  # Extract inputs from list/tuple\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = feature_extractor(inputs)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs.cpu())\n",
        "\n",
        "    features = torch.cat(features, dim=0)\n",
        "    return features\n",
        "\n",
        "# Classifier model\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=10):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Main training and evaluation function\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Data transformations\n",
        "    data_transforms = transforms.Compose([\n",
        "        # Removed transforms.ToPILImage() since images are already PIL Images\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Feature extractor\n",
        "    feature_extractor = models.resnet152(pretrained=True)\n",
        "    feature_extractor = nn.Sequential(*list(feature_extractor.children())[:-1])\n",
        "    feature_extractor = feature_extractor.to(device)\n",
        "    feature_extractor.eval()\n",
        "\n",
        "\n",
        "    # Paths to datasets\n",
        "    dataset_folder = 'dataset'\n",
        "    part_one_dataset = os.path.join(dataset_folder, 'part_one_dataset')\n",
        "    part_two_dataset = os.path.join(dataset_folder, 'part_two_dataset')\n",
        "\n",
        "    # Load D1 (dataset index 1 in part_one_dataset)\n",
        "    D1_train_data_path = os.path.join(part_one_dataset, 'train_data', '1_train_data.tar.pth')\n",
        "    D1_data, D1_labels = load_dataset(D1_train_data_path)\n",
        "    D1_dataset = CIFARDataset(D1_data, D1_labels, transform=data_transforms)\n",
        "    D1_dataloader = DataLoader(D1_dataset, batch_size=64, shuffle=False)\n",
        "    D1_features, D1_labels = extract_features(D1_dataloader, feature_extractor, device)\n",
        "\n",
        "    # Initialize model f1\n",
        "    input_dim = D1_features.size(1)\n",
        "    f_prev = Classifier(input_dim, num_classes=10).to(device)\n",
        "\n",
        "    # Train f1 on D1\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(f_prev.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    num_epochs = 20\n",
        "    dataset = torch.utils.data.TensorDataset(D1_features, D1_labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    print(\"Training model f1 on D1\")\n",
        "\n",
        "    # Loss components\n",
        "    criterion_ce = nn.CrossEntropyLoss()  # For pseudo-labels\n",
        "    temperature = 1.0  # Distillation temperature\n",
        "    lambda_distill = 0.1  # Weight for distillation loss\n",
        "    lambda_reg = 0.01  # Weight for L2 regularization\n",
        "\n",
        "    theta_prev = {name: param.detach().clone() for name, param in f_prev.named_parameters()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        f_prev.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Current model predictions\n",
        "            outputs_current = f_prev(inputs)\n",
        "            loss_ce = criterion_ce(outputs_current, labels)\n",
        "\n",
        "            # Previous model predictions (soft targets)\n",
        "            with torch.no_grad():\n",
        "                outputs_prev = f_prev(inputs)\n",
        "            soft_targets_prev = torch.softmax(outputs_prev / temperature, dim=1)\n",
        "            soft_targets_current = torch.log_softmax(outputs_current / temperature, dim=1)\n",
        "            loss_distill = -torch.sum(soft_targets_prev * soft_targets_current) / inputs.size(0)\n",
        "\n",
        "            # L2 regularization\n",
        "            l2_reg = 0.0\n",
        "            for name, param in f_prev.named_parameters():\n",
        "                l2_reg += torch.norm(param - theta_prev[name].to(device)) ** 2\n",
        "\n",
        "            # Combined loss\n",
        "            loss = loss_ce + lambda_distill * loss_distill + lambda_reg * l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}')\n",
        "\n",
        "    # Initialize accuracy matrix\n",
        "    # accuracy_matrix = np.zeros((20, 20))\n",
        "\n",
        "    # Loop over datasets D2 to D10\n",
        "    for i in range(1, 11):\n",
        "        print(f'\\nProcessing Dataset D{i}')\n",
        "\n",
        "        if i <= 10:\n",
        "            # Datasets D2 to D10 in part_one_dataset\n",
        "            train_data_path = os.path.join(part_one_dataset, 'train_data', f'{i}_train_data.tar.pth')\n",
        "        else:\n",
        "            # Datasets D11 to D20 in part_two_dataset\n",
        "            idx = i - 10  # Since files are numbered from 1 to 10\n",
        "            train_data_path = os.path.join(part_two_dataset, 'train_data', f'{idx}_train_data.tar.pth')\n",
        "\n",
        "        # Load Di\n",
        "        Di_data, _ = load_dataset(train_data_path)\n",
        "        Di_dataset = CIFARDataset(Di_data, transform=data_transforms)\n",
        "        Di_dataloader = DataLoader(Di_dataset, batch_size=32, shuffle=False)\n",
        "        Di_features = extract_features_unlabeled(Di_dataloader, feature_extractor, device)\n",
        "\n",
        "        # Predict labels using f_prev\n",
        "        f_prev.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = f_prev(Di_features.to(device))\n",
        "            _, predicted_labels = torch.max(outputs, 1)\n",
        "            predicted_labels = predicted_labels.cpu()\n",
        "\n",
        "        # Prepare dataset with pseudo-labels\n",
        "        dataset = torch.utils.data.TensorDataset(Di_features, predicted_labels)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        # Initialize current model fi\n",
        "        fi = Classifier(input_dim, num_classes=10).to(device)\n",
        "        fi.load_state_dict(f_prev.state_dict())\n",
        "\n",
        "# Store the parameters of the previous model (f_prev)\n",
        "        theta_prev = {name: param.detach().clone() for name, param in f_prev.named_parameters()}\n",
        "\n",
        "\n",
        "        # Regularization parameter\n",
        "        lambda_reg = 1.0 if i <= 10 else 0.1  # Adjusted for potential distribution shift\n",
        "\n",
        "        # Store previous parameters\n",
        "        for name, param in f_prev.named_parameters():\n",
        "            theta_prev[name] = param.detach().clone()\n",
        "\n",
        "\n",
        "                # Initialize optimizer and scheduler\n",
        "        optimizer = optim.Adam(fi.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "        # Training fi on Di with pseudo-labels\n",
        "        print(f\"Training model f{i} on D{i}\")\n",
        "        for epoch in range(num_epochs):\n",
        "            fi.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Predictions from f_prev (teacher) for distillation\n",
        "                with torch.no_grad():\n",
        "                    outputs_prev = f_prev(inputs)\n",
        "\n",
        "                # Predictions from fi (student)\n",
        "                outputs_current = fi(inputs)\n",
        "\n",
        "                # Cross-entropy loss\n",
        "                loss_ce = criterion(outputs_current, labels)\n",
        "\n",
        "                # Distillation loss\n",
        "                soft_targets_prev = torch.softmax(outputs_prev / temperature, dim=1)\n",
        "                soft_targets_current = torch.log_softmax(outputs_current / temperature, dim=1)\n",
        "                loss_distill = -torch.sum(soft_targets_prev * soft_targets_current) / inputs.size(0)\n",
        "\n",
        "                # Regularization loss\n",
        "                l2_reg = 0.0\n",
        "                for name, param in fi.named_parameters():\n",
        "                    l2_reg += torch.norm(param - theta_prev[name].to(device)) ** 2\n",
        "\n",
        "                # Combined loss\n",
        "                loss = loss_ce + lambda_distill * loss_distill + lambda_reg * l2_reg\n",
        "\n",
        "                # Backpropagation\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Update learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Evaluation on D_hat datasets\n",
        "        for j in range(1, i+1):\n",
        "            if j <= 10:\n",
        "                # D_hat datasets in part_one_dataset\n",
        "                eval_data_path = os.path.join(part_one_dataset, 'eval_data', f'{j}_eval_data.tar.pth')\n",
        "                D_hat_data, D_hat_labels = load_dataset(eval_data_path)\n",
        "            else:\n",
        "                # D_hat datasets in part_two_dataset\n",
        "                idx = j - 10\n",
        "                eval_data_path = os.path.join(part_two_dataset, 'eval_data', f'{idx}_eval_data.tar.pth')\n",
        "                D_hat_data, D_hat_labels = load_dataset(eval_data_path)\n",
        "\n",
        "            D_hat_dataset = CIFARDataset(D_hat_data, D_hat_labels, transform=data_transforms)\n",
        "            D_hat_dataloader = DataLoader(D_hat_dataset, batch_size=64, shuffle=False)\n",
        "            D_hat_features, D_hat_labels = extract_features(D_hat_dataloader, feature_extractor, device)\n",
        "\n",
        "            fi.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = fi(D_hat_features.to(device))\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct = (predicted.cpu() == D_hat_labels).sum().item()\n",
        "                total = D_hat_labels.size(0)\n",
        "                accuracy = correct / total\n",
        "                print(f'Model f{i} Accuracy on D_hat{j}: {accuracy*100:.2f}%')\n",
        "\n",
        "                # Store accuracy in matrix\n",
        "                accuracy_matrix[i-1][j-1] = accuracy * 100\n",
        "\n",
        "        # Update f_prev to fi\n",
        "        f_prev = fi\n",
        "\n",
        "    # Display accuracy matrix\n",
        "    print('\\nAccuracy Matrix:')\n",
        "    for i in range(20):\n",
        "        accuracies = ['{:.2f}'.format(acc) for acc in accuracy_matrix[i][:i+1]]\n",
        "        print(f'Model f{i+1}: ' + ', '.join(accuracies))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "btnx5WLlqyzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}